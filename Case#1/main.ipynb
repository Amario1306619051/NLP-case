{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksplorasi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.prepos_text import preprocess_dataframe\n",
    "\n",
    "\n",
    "dataset = preprocess_dataframe('/home/rnd/Downloads/dataset_tweet_sentiment_cellular_service_provider.csv', remove_stopword=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      positive\n",
       "1      positive\n",
       "2      negative\n",
       "3      negative\n",
       "4      negative\n",
       "         ...   \n",
       "295    positive\n",
       "296    positive\n",
       "297    positive\n",
       "298    positive\n",
       "299    negative\n",
       "Name: Sentiment, Length: 300, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {label: idx for idx, label in enumerate(dataset['Sentiment'].unique())}\n",
    "labels = dataset['Sentiment'].map(label_dict).fillna(-1).astype(int).values\n",
    "label_dict = {idx: label for label, idx in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'positive', 1: 'negative'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'positive', 1: 'negative'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.split import split_dataset, split_dataset_stratisfied\n",
    "train_texts, test_texts, train_labels, test_labels = split_dataset(texts=dataset['processed_text'].values, labels=labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.dataloader import nltk_tokenizer, vocabularize, build_dataloader\n",
    "\n",
    "\n",
    "vocab = vocabularize(\n",
    "    texts=dataset['processed_text'].values\n",
    ")\n",
    "train_dataloader = build_dataloader(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer=nltk_tokenizer,\n",
    "    vocab=vocab,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_dataloader = build_dataloader(\n",
    "    texts=test_texts,\n",
    "    labels=test_labels,\n",
    "    tokenizer=nltk_tokenizer,\n",
    "    vocab=vocab,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'negative']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentiment'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.LSTM import LSTMModel3, BidirectionalLSTMModel, GRUModel, LSTM_CNN_Model\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rnd/Documents/Belajar/Nawatech/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 0.6953, Acc: 51.25% | Val Loss: 0.6953, Acc: 50.00% | F1: 0.3844, Prec: 0.4070, Rec: 0.4613\n",
      "Epoch 2/100 | Train Loss: 0.6945, Acc: 54.58% | Val Loss: 0.6966, Acc: 50.00% | F1: 0.3844, Prec: 0.4070, Rec: 0.4613\n",
      "Epoch 3/100 | Train Loss: 0.6869, Acc: 56.25% | Val Loss: 0.6979, Acc: 46.67% | F1: 0.4360, Prec: 0.4403, Rec: 0.4478\n",
      "Epoch 4/100 | Train Loss: 0.6831, Acc: 60.42% | Val Loss: 0.6988, Acc: 48.33% | F1: 0.3943, Prec: 0.4100, Rec: 0.4495\n",
      "Epoch 5/100 | Train Loss: 0.5866, Acc: 70.00% | Val Loss: 0.6723, Acc: 65.00% | F1: 0.6328, Prec: 0.6500, Rec: 0.6347\n",
      "Epoch 6/100 | Train Loss: 0.3627, Acc: 85.00% | Val Loss: 0.6515, Acc: 78.33% | F1: 0.7758, Prec: 0.7907, Rec: 0.7727\n",
      "Epoch 7/100 | Train Loss: 0.1939, Acc: 94.17% | Val Loss: 0.5291, Acc: 78.33% | F1: 0.7783, Prec: 0.7847, Rec: 0.7761\n",
      "Epoch 8/100 | Train Loss: 0.1379, Acc: 96.67% | Val Loss: 0.6780, Acc: 75.00% | F1: 0.7494, Prec: 0.7708, Rec: 0.7626\n",
      "Epoch 9/100 | Train Loss: 0.0496, Acc: 98.33% | Val Loss: 0.6236, Acc: 83.33% | F1: 0.8264, Prec: 0.8498, Rec: 0.8215\n",
      "Epoch 10/100 | Train Loss: 0.0385, Acc: 97.92% | Val Loss: 0.7674, Acc: 85.00% | F1: 0.8496, Prec: 0.8500, Rec: 0.8535\n",
      "Epoch 11/100 | Train Loss: 0.0018, Acc: 100.00% | Val Loss: 0.7176, Acc: 86.67% | F1: 0.8653, Prec: 0.8653, Rec: 0.8653\n",
      "Epoch 12/100 | Train Loss: 0.0007, Acc: 100.00% | Val Loss: 0.7918, Acc: 83.33% | F1: 0.8286, Prec: 0.8402, Rec: 0.8249\n",
      "Epoch 13/100 | Train Loss: 0.0006, Acc: 100.00% | Val Loss: 0.8484, Acc: 83.33% | F1: 0.8286, Prec: 0.8402, Rec: 0.8249\n",
      "Epoch 14/100 | Train Loss: 0.0003, Acc: 100.00% | Val Loss: 0.8699, Acc: 83.33% | F1: 0.8286, Prec: 0.8402, Rec: 0.8249\n",
      "Epoch 15/100 | Train Loss: 0.0006, Acc: 100.00% | Val Loss: 0.9496, Acc: 81.67% | F1: 0.8124, Prec: 0.8194, Rec: 0.8098\n",
      "Epoch 16/100 | Train Loss: 0.0002, Acc: 100.00% | Val Loss: 0.9546, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 17/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 0.9632, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 18/100 | Train Loss: 0.0002, Acc: 100.00% | Val Loss: 0.9783, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 19/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 0.9825, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 20/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 0.9806, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 21/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0559, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 22/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 0.9834, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 23/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 0.9443, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 24/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0509, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 25/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 1.0538, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 26/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 0.9597, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 27/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0412, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 28/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0255, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 29/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 1.0545, Acc: 83.33% | F1: 0.8303, Prec: 0.8343, Rec: 0.8283\n",
      "Epoch 30/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0626, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 31/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0856, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 32/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0327, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 33/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 1.0342, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 34/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0096, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 35/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0470, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 36/100 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 1.0125, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 37/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0307, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 38/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0197, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 39/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0107, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 40/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0215, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 41/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0203, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 42/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0452, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 43/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0041, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n",
      "Epoch 44/100 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 1.0573, Acc: 85.00% | F1: 0.8479, Prec: 0.8495, Rec: 0.8468\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Panggil langsung\u001b[39;00m\n\u001b[32m      2\u001b[39m model = LSTM_CNN_Model()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madam\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# tinggal pilih optimizer di sini\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDL_models/LSTM_CNN_Model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnltk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Belajar/Nawatech/Case#1/model/BaseLSTM.py:238\u001b[39m, in \u001b[36mSimpleLSTMModel2.train_model\u001b[39m\u001b[34m(self, train_dataloader, val_dataloader, labels, criterion, optimizer, model_path, num_epochs, vocab, embed_size, hidden_size, num_layers, dropout, tokenizer_name)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[32m    237\u001b[39m     optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m    240\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Belajar/Nawatech/Case#1/model/LSTM.py:171\u001b[39m, in \u001b[36mLSTM_CNN_Model.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[33;03mForward pass for LSTM + CNN model.\u001b[39;00m\n\u001b[32m    163\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    168\u001b[39m \u001b[33;03m    Tensor: Logits for each class.\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m embedded = \u001b[38;5;28mself\u001b[39m.embedding(x)                             \u001b[38;5;66;03m# (batch_size, seq_len, embed_size)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m lstm_out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m                        \u001b[38;5;66;03m# (batch_size, seq_len, hidden_size)\u001b[39;00m\n\u001b[32m    172\u001b[39m lstm_out = lstm_out.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)                     \u001b[38;5;66;03m# (batch_size, hidden_size, seq_len)\u001b[39;00m\n\u001b[32m    173\u001b[39m cnn_out = \u001b[38;5;28mself\u001b[39m.conv(lstm_out)                            \u001b[38;5;66;03m# (batch_size, hidden_size, seq_len)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Belajar/Nawatech/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Belajar/Nawatech/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Belajar/Nawatech/env/lib/python3.12/site-packages/torch/nn/modules/rnn.py:878\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m    875\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     result = _VF.lstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m._flat_weights, \u001b[38;5;28mself\u001b[39m.bias,\n\u001b[32m    882\u001b[39m                       \u001b[38;5;28mself\u001b[39m.num_layers, \u001b[38;5;28mself\u001b[39m.dropout, \u001b[38;5;28mself\u001b[39m.training, \u001b[38;5;28mself\u001b[39m.bidirectional)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Panggil langsung\n",
    "model = LSTM_CNN_Model()\n",
    "model.train_model(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    labels=label_dict,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=\"adam\",  # tinggal pilih optimizer di sini\n",
    "    model_path=\"DL_models/LSTM_CNN_Model\",\n",
    "    num_epochs=100,\n",
    "    vocab=vocab,\n",
    "    embed_size=256,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.5,\n",
    "    tokenizer_name=\"nltk\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'positive', 0.5168493986129761)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_CNN_Model(weights='/home/rnd/Documents/Belajar/Nawatech/Case#1/DL_models/LSTM_CNN_Model/best.pt')\n",
    "model.predict('cakep banget woy sukaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Acc: 86.67% | F1: 0.8653, Prec: 0.8653, Rec: 0.8653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 86.66666666666667,\n",
       " 'precision': 0.8653198653198653,\n",
       " 'recall': 0.8653198653198653,\n",
       " 'f1_score': 0.8653198653198653}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.valid(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
